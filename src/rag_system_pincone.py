# -*- coding: utf-8 -*-
"""Rag_system_pincone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-vCfbbe-pGc1h2PIHwHlLTtKE25tolWT
"""

import os
import pandas as pd
from sentence_transformers import SentenceTransformer
import numpy as np
from transformers import pipeline
from pinecone import Pinecone, ServerlessSpec
from evaluate import load


api_key = "a568d38a-836e-4c3e-bc9a-a5e523215908"
pc = Pinecone(api_key=api_key)

index_name = "airbnb-properties"
dimension = 384

# create index
if index_name not in [index_info["name"] for index_info in pc.list_indexes()]:
    pc.create_index(
        name=index_name,
        dimension=dimension,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

index = pc.Index(index_name)

# df = pd.read_csv("../data/cleaned_airbnb_data.csv")
# df = pd.read_csv("data/cleaned_airbnb_data.csv")
df = pd.read_csv("./data/cleaned_airbnb_data.csv")
df['description'] = df['description'].fillna('').astype(str)
descriptions = df['description'].tolist()

# Preparing and embedding property descriptions using a pre-trained language model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = embedding_model.encode(descriptions, convert_to_tensor=True).cpu().detach().numpy()

# Upserting embeddings to the Pinecone index
batch_size = 50
for i in range(0, len(descriptions), batch_size):

    batch_upserts = [(str(j), embeddings[j].tolist()) for j in range(i, min(i + batch_size, len(descriptions)))]
    index.upsert(vectors=batch_upserts)
    print(f"Upserted batch {i // batch_size + 1} of {len(descriptions) // batch_size + 1}")

print("All embeddings upserted successfully.")

# retrieval function
def retrieve_properties(query, k=10):
    """
    Retrieves the top-k most similar property descriptions based on a user query using Pinecone.

    Args:
        query (str): A query string from the user.
        k (int): The number of top results to retrieve.

    Returns:
        list: A list of the top-k retrieved property descriptions.
    """
    query_embedding = embedding_model.encode(query).tolist()
    results = index.query(vector=query_embedding, top_k=k)

    # Retrieve descriptions based on IDs from the query results, with error handling
    retrieved_descriptions = []
    for match in results['matches']:
        try:
            idx = int(match['id'])
            if 0 <= idx < len(descriptions):
                retrieved_descriptions.append(descriptions[idx])
            else:
                print(f"Warning: Retrieved ID {idx} is out of bounds.")
        except ValueError:
            print(f"Warning: Retrieved ID {match['id']} could not be converted to an integer.")
    return retrieved_descriptions

# generate responses based on the retrieved data
generator = pipeline("text-generation", model="distilgpt2")

# Basic LLM model answer generation function
def generate_llm_answer(question):
    """
    Generates an answer using the basic LLM model for a given question.
    """
    response = generator(question, max_new_tokens=50, num_return_sequences=1)
    return response[0]['generated_text'].strip()

# RAG model answer generation function
def generate_rag_answer(question):
    """
    Generates an answer using the RAG model by first retrieving relevant property descriptions.
    """
    relevant_descriptions = retrieve_properties(question, k=3)  # Top 3 relevant descriptions
    prompt = " ".join(relevant_descriptions) + f"\nQuestion: {question}\nAnswer:"
    response = generator(prompt, max_new_tokens=50, num_return_sequences=1)
    return response[0]['generated_text'].strip()

# Generate response function using the retrieved descriptions
def generate_response_with_qa(query):
    relevant_descriptions = retrieve_properties(query, k=5)
    if not relevant_descriptions:
        return "No relevant descriptions found."

    # Construct prompt with descriptions
    prompt = "Based on these property descriptions, suggest a suitable property:\n"
    prompt += "\n".join(relevant_descriptions[:3])

    # Generate the response
    response = generator(prompt, max_new_tokens=50, num_return_sequences=1, truncation=True)
    return response[0]['generated_text']

# Load synthetic QA pairs for evaluation
qa_df = pd.read_csv("./data/synthetic_qa_pairs.csv")
questions = qa_df['synthetic_question'].tolist()
reference_answers = qa_df['synthetic_answer'].tolist()

# Evaluation function for comparing LLM and RAG models
def evaluate_models(questions, reference_answers):
    bleu_metric = load("bleu")
    rouge_metric = load("rouge")
    bertscore_metric = load("bertscore")

    # Basic LLM model
    llm_generated_answers = [generate_llm_answer(q) for q in questions]
    llm_bleu = bleu_metric.compute(predictions=llm_generated_answers, references=[[ref] for ref in reference_answers])
    llm_rouge = rouge_metric.compute(predictions=llm_generated_answers, references=reference_answers)
    llm_bertscore = bertscore_metric.compute(predictions=llm_generated_answers, references=reference_answers, lang="en")

    # RAG model with retrieved properties
    rag_generated_answers = [generate_rag_answer(q) for q in questions]
    rag_bleu = bleu_metric.compute(predictions=rag_generated_answers, references=[[ref] for ref in reference_answers])
    rag_rouge = rouge_metric.compute(predictions=rag_generated_answers, references=reference_answers)
    rag_bertscore = bertscore_metric.compute(predictions=rag_generated_answers, references=reference_answers, lang="en")

    print("LLM Model Performance:")
    print(f"BLEU: {llm_bleu['bleu']:.2f}")
    print(f"ROUGE-1: {llm_rouge['rouge1']:.2f}")
    print(f"BERTScore F1: {np.mean(llm_bertscore['f1']):.2f}\n")

    print("RAG Model Performance:")
    print(f"BLEU: {rag_bleu['bleu']:.2f}")
    print(f"ROUGE-1: {rag_rouge['rouge1']:.2f}")
    print(f"BERTScore F1: {np.mean(rag_bertscore['f1']):.2f}")

# evaluation
evaluate_models(questions[:10], reference_answers[:10])
