# -*- coding: utf-8 -*-
"""Rag_system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-vCfbbe-pGc1h2PIHwHlLTtKE25tolWT
"""

!pip install faiss-cpu

import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from transformers import pipeline
from sklearn.metrics import precision_score, recall_score

# Load Airbnb dataset with property descriptions and embeddings
df = pd.read_json("hf://datasets/MongoDB/airbnb_embeddings/airbnb_embeddings.json", lines=True)
print("Columns in the dataset:", df.columns)

# Load synthetic QA pairs dataset for question-answer reference
qa_df = pd.read_csv("/content/synthetic_qa_pairs_cohere_sample.csv")
print("Columns in synthetic QA file:", qa_df.columns)

# Prepare and embed property descriptions using a pre-trained language model
descriptions = df['description'].tolist()
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = embedding_model.encode(descriptions, convert_to_tensor=True).cpu().detach().numpy()

# Initialize and populate a FAISS index with property description embeddings
d = embeddings.shape[1]
index = faiss.IndexFlatL2(d)
index.add(embeddings.astype('float32'))

# Retrieve function: retrieves top-k similar property descriptions based on the query
def retrieve_properties(query, k=10):
    """
    Retrieves the top-k most similar property descriptions based on a user query using FAISS.

    Args:
        query (str): A query string from the user.
        k (int): The number of top results to retrieve.

    Returns:
        list: A list of the top-k retrieved property descriptions.
    """
    query_embedding = embedding_model.encode(query, convert_to_tensor=True).cpu().detach().numpy()
    query_embedding = np.expand_dims(query_embedding, axis=0).astype('float32')
    distances, indices = index.search(query_embedding, k)
    return [descriptions[idx] for idx in indices[0]]

# Load text generation model to generate responses based on the retrieved data
generator = pipeline("text-generation", model="distilgpt2")

# Generate response function: uses retrieved descriptions and synthetic QA pairs to create an answer
def generate_response_with_qa(query):
    """
    Generates a response based on retrieved property descriptions and relevant QA pairs.

    Args:
        query (str): A user query to generate a response for.

    Returns:
        str: A generated response combining property descriptions and QA pairs.
    """
    relevant_descriptions = retrieve_properties(query, k=5)
    if not relevant_descriptions:
        return "No relevant descriptions found."

    # Retrieve relevant synthetic QA pairs related to the query
    relevant_qa_pairs = qa_df[qa_df['synthetic_question'].str.contains(query, case=False, na=False)]

    # Construct a prompt for generation by combining descriptions and QA pairs
    prompt = "Based on these property descriptions and example Q&A, describe a suitable property:\n"
    prompt += "\n".join(relevant_descriptions[:3])
    prompt += "\nExample Q&A:\n"
    prompt += "\n".join([f"Q: {row['synthetic_question']}\nA: {row['synthetic_answer']}" for _, row in relevant_qa_pairs.iterrows()][:3])

    # Generate a response using the text generation model
    response = generator(prompt, max_new_tokens=50, num_return_sequences=1, truncation=True)
    return response[0]['generated_text']

# Rerank results based on user preferences
def rerank_results(results, preferences):
    """
    Reranks retrieved results based on the user's preferences.

    Args:
        results (list): A list of retrieved property descriptions.
        preferences (list): A list of words the user prefers, e.g., "wheelchair", "elevator".

    Returns:
        list: The ranked results based on the user's preferences.
    """
    ranked_results = sorted(results, key=lambda x: sum(1 for word in preferences if word in x.lower()), reverse=True)
    return ranked_results

# Evaluation function: calculates precision and recall between true answers and generated answers
def evaluate_rag_system(true_answers, generated_answers):
    """
    Evaluates the RAG system's performance using precision and recall metrics.

    Args:
        true_answers (list of int): A list of binary ground truth labels (1 for relevant, 0 for non-relevant).
        generated_answers (list of int): A list of binary predicted labels by the RAG system (1 for relevant, 0 for non-relevant).

    Prints:
        Precision and Recall scores.
    """
    precision = precision_score(true_answers, generated_answers, average='binary')
    recall = recall_score(true_answers, generated_answers, average='binary')
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")

# Example usage demonstrating the functions in the workflow
query = "Looking for a wheelchair accessible property with an elevator."
retrieved_properties = retrieve_properties(query, k=10)
response = generate_response_with_qa(query)
print("Generated Response with QA:", response)

preferences = ["wheelchair", "elevator", "accessible"]
ranked_results = rerank_results(retrieved_properties, preferences)
print("Ranked Results:", ranked_results[:5])

# Example evaluation of the RAG system with dummy data
true_answers = [1, 0, 1, 1, 0]
generated_answers = [1, 0, 1, 0, 0]
evaluate_rag_system(true_answers, generated_answers)

from sklearn.metrics import precision_score, recall_score, f1_score

def evaluate_rag_system(true_answers, generated_answers):
    """
    Evaluates the performance of a Retrieval-Augmented Generation (RAG) system using precision, recall, and F1-score metrics.

    This function compares the ground truth answers to the answers generated by the RAG system
    and calculates precision, recall, and F1-score.

    Args:
        true_answers (list of int): A list of binary ground truth labels (1 for relevant, 0 for non-relevant).
        generated_answers (list of int): A list of binary predicted labels by the RAG system (1 for relevant, 0 for non-relevant).

    Prints:
        Precision (float): The proportion of relevant documents retrieved by the RAG system.
        Recall (float): The proportion of relevant documents retrieved out of all possible relevant documents.
        F1-Score (float): The harmonic mean of precision and recall.

    Example:
        true_answers = [1, 0, 1, 1, 0]
        generated_answers = [1, 0, 1, 0, 0]
        evaluate_rag_system(true_answers, generated_answers)

    """
    precision = precision_score(true_answers, generated_answers, average='binary')
    recall = recall_score(true_answers, generated_answers, average='binary')
    f1 = f1_score(true_answers, generated_answers, average='binary')

    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1-Score: {f1:.2f}")


# Example true answers and generated answers
true_answers = [1, 0, 1, 1, 0]
generated_answers = [1, 0, 1, 0, 0]

# Evaluate the RAG system
evaluate_rag_system(true_answers, generated_answers)