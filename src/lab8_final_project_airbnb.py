# -*- coding: utf-8 -*-
"""lab8_final_project_airbnb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cyVFlycjz22YCosXNrE7yNAiOPp_KUF_
"""

!pip install datasets

from datasets import load_dataset

# Load the Airbnb embeddings dataset
dataset = load_dataset("MongoDB/airbnb_embeddings")

"""Exploring the data"""

# Check the structure of the dataset
print(dataset)

# Display the first record to see the available fields
print(dataset["train"][0])

# Display only the field names
print(dataset["train"].column_names)

# Look at specific fields for the first record
fields_of_interest = ["description", "amenities", "text_embeddings"]
sample_record = {field: dataset["train"][0][field] for field in fields_of_interest}
print(sample_record)

# Show multiple entries for selected fields
for i in range(3):  # adjust range to show more/less
    print({field: dataset["train"][i][field] for field in fields_of_interest})

# Check for missing values in each field
missing_values = {col: dataset["train"].filter(lambda x: x[col] is None).num_rows for col in dataset["train"].column_names}
print(missing_values)

from datasets import Dataset
from math import isnan  # Import isnan for checking NaN values

# Helper function to check if a value is considered "missing"
def is_missing(value, missing_indicators):
    if isinstance(value, list):
        return all(is_missing(item, missing_indicators) for item in value)
    if isinstance(value, dict):
        return all(is_missing(v, missing_indicators) for v in value.values())
    if value in missing_indicators:
        return True
    if isinstance(value, float) and isnan(value):
        return True
    if isinstance(value, str) and value.strip() == "":
        return True
    return False

# Function to check for missing values with expanded conditions and handling for nested structures
def check_missing_values(dataset):
    missing_indicators = {None, "", "N/A", "none", "null", "NULL"}  # Set of missing value indicators
    missing_values = {}

    for col in dataset["train"].column_names:
        missing_count = sum(1 for x in dataset["train"][col] if is_missing(x, missing_indicators))
        missing_values[col] = missing_count
    return missing_values

# Get missing values for each column
missing_values = check_missing_values(dataset)
print(missing_values)

"""Data preprocessing"""

# Drop the 'weekly_price' and 'monthly_price' fields
dataset["train"] = dataset["train"].remove_columns(["weekly_price", "monthly_price"])

import numpy as np

# Calculate the average for relevant numeric fields using NumPy
avg_bedrooms = np.mean([x for x in dataset["train"]["bedrooms"] if x is not None])
avg_beds = np.mean([x for x in dataset["train"]["beds"] if x is not None])
avg_security_deposit = np.mean([x for x in dataset["train"]["security_deposit"] if x is not None])
avg_cleaning_fee = np.mean([x for x in dataset["train"]["cleaning_fee"] if x is not None])

# Check for missing values in relevant fields
missing_values_count = {
    'first_review': dataset["train"].filter(lambda x: x['first_review'] is None).num_rows,
    'last_review': dataset["train"].filter(lambda x: x['last_review'] is None).num_rows,
    'bedrooms': dataset["train"].filter(lambda x: x['bedrooms'] is None).num_rows,
    'beds': dataset["train"].filter(lambda x: x['beds'] is None).num_rows,
    'security_deposit': dataset["train"].filter(lambda x: x['security_deposit'] is None).num_rows,
    'cleaning_fee': dataset["train"].filter(lambda x: x['cleaning_fee'] is None).num_rows,
}

print(missing_values_count)

# Function to identify non-numeric entries
def find_non_numeric_entries(field):
    non_numeric_entries = [
        record[field] for record in dataset["train"] if not isinstance(record[field], (int, float)) and record[field] is not None
    ]
    return non_numeric_entries

# Check for non-numeric entries in relevant fields
non_numeric_counts = {
    'bedrooms': find_non_numeric_entries('bedrooms'),
    'beds': find_non_numeric_entries('beds'),
    'security_deposit': find_non_numeric_entries('security_deposit'),
    'cleaning_fee': find_non_numeric_entries('cleaning_fee'),
}

# Print counts of non-numeric entries
for field, entries in non_numeric_counts.items():
    print(f"{field} non-numeric entries: {entries}")

def clean_non_numeric_entries(record):
    # Convert to numeric or fill with a default value
    if record['bedrooms'] is not None and not isinstance(record['bedrooms'], (int, float)):
        record['bedrooms'] = None  # or some default value
    if record['beds'] is not None and not isinstance(record['beds'], (int, float)):
        record['beds'] = None  # or some default value
    if record['security_deposit'] is not None and not isinstance(record['security_deposit'], (int, float)):
        record['security_deposit'] = None  # or some default value
    if record['cleaning_fee'] is not None and not isinstance(record['cleaning_fee'], (int, float)):
        record['cleaning_fee'] = None  # or some default value
    return record

# Apply the cleaning function
dataset["train"] = dataset["train"].map(clean_non_numeric_entries)

# Check for NaN values in relevant fields
nan_counts = {
    'first_review': dataset["train"].filter(lambda x: x['first_review'] is None).num_rows,
    'last_review': dataset["train"].filter(lambda x: x['last_review'] is None).num_rows,
    'bedrooms': dataset["train"].filter(lambda x: x['bedrooms'] is None).num_rows,
    'beds': dataset["train"].filter(lambda x: x['beds'] is None).num_rows,
    'security_deposit': dataset["train"].filter(lambda x: x['security_deposit'] is None).num_rows,
    'cleaning_fee': dataset["train"].filter(lambda x: x['cleaning_fee'] is None).num_rows,
}

print("NaN Counts:")
for field, count in nan_counts.items():
    print(f"{field}: {count}")

# Recalculate averages in case any rows have been modified
avg_bedrooms = np.mean([x for x in dataset["train"]["bedrooms"] if x is not None])
avg_beds = np.mean([x for x in dataset["train"]["beds"] if x is not None])
avg_security_deposit = np.mean([x for x in dataset["train"]["security_deposit"] if x is not None])
avg_cleaning_fee = np.mean([x for x in dataset["train"]["cleaning_fee"] if x is not None])

print(f"Averages - Bedrooms: {avg_bedrooms}, Beds: {avg_beds}, Security Deposit: {avg_security_deposit}, Cleaning Fee: {avg_cleaning_fee}")

# Check field types in the dataset
field_types = {
    'bedrooms': dataset["train"].features['bedrooms'],
    'beds': dataset["train"].features['beds'],
    'security_deposit': dataset["train"].features['security_deposit'],
    'cleaning_fee': dataset["train"].features['cleaning_fee'],
}

print("Field Types:")
for field, dtype in field_types.items():
    print(f"{field}: {dtype}")

# Sample a few records from the dataset to inspect numeric fields
sample_records = dataset["train"].select(range(10))  # Adjust the range as needed
print("Sample Records:")
print(sample_records)

# Check data types of all features
print("Dataset Features and Their Types:")
for feature, dtype in dataset["train"].features.items():
    print(f"{feature}: {dtype}")

# Sample values from problematic fields to find non-numeric entries
problematic_fields = ['bedrooms', 'beds', 'security_deposit', 'cleaning_fee']

for field in problematic_fields:
    non_numeric_entries = dataset["train"].filter(lambda x: isinstance(x[field], str))
    print(f"Non-numeric entries in {field}:")
    print(non_numeric_entries)

import pandas as pd
import numpy as np
from datasets import Dataset

# Convert dataset to pandas DataFrame for easier manipulation
df = dataset['train'].to_pandas()

# Step 1: Drop unnecessary columns if they exist
columns_to_drop = ['weekly_price', 'monthly_price']
columns_to_drop = [col for col in columns_to_drop if col in df.columns]  # Only drop if they exist
df.drop(columns=columns_to_drop, inplace=True)

# Step 2: Calculate average values for numeric fields
average_bedrooms = df['bedrooms'].mean()
average_beds = df['beds'].mean()
average_security_deposit = df['security_deposit'].mean()
average_cleaning_fee = df['cleaning_fee'].mean()

# Step 3: Fill missing values
def fill_missing_values(row):
    # Check and fill 'first_review' and 'last_review' with NaT for consistency
    if pd.isnull(row['first_review']):
        row['first_review'] = pd.NaT  # Set to NaT for datetime compatibility
    if pd.isnull(row['last_review']):
        row['last_review'] = pd.NaT  # Set to NaT for datetime compatibility

    # For numeric fields, check for NaN and fill with averages
    if pd.isnull(row['number_of_reviews']):
        row['number_of_reviews'] = 0  # Use a numeric value (0 for no reviews)
    if pd.isnull(row['bedrooms']):
        row['bedrooms'] = average_bedrooms
    if pd.isnull(row['beds']):
        row['beds'] = average_beds
    if pd.isnull(row['security_deposit']):
        row['security_deposit'] = average_security_deposit
    if pd.isnull(row['cleaning_fee']):
        row['cleaning_fee'] = average_cleaning_fee
    return row

# Apply the function to fill missing values
df = df.apply(fill_missing_values, axis=1)

# Ensure 'first_review' and 'last_review' are of datetime type
df['first_review'] = pd.to_datetime(df['first_review'], errors='coerce')
df['last_review'] = pd.to_datetime(df['last_review'], errors='coerce')

# Check the data types after filling missing values
print("Data types after filling missing values:")
print(df.dtypes)

# Convert back to Dataset format
dataset['train'] = Dataset.from_pandas(df)

# Verify the changes
print("Updated dataset sample:")
print(dataset['train'].to_pandas().head())

# Inspect unique values in 'first_review' and 'last_review' to identify issues
print("Unique values in 'first_review':")
print(df['first_review'].unique())
print("Unique values in 'last_review':")
print(df['last_review'].unique())

# Step 1: Handle 'first_review' and 'last_review' specifically
# Convert to datetime and handle errors
def convert_to_datetime(value):
    try:
        return pd.to_datetime(value)
    except Exception:
        return None  # or return pd.NaT for missing datetime

# Apply conversion
df['first_review'] = df['first_review'].apply(convert_to_datetime)
df['last_review'] = df['last_review'].apply(convert_to_datetime)

# Step 2: Fill missing values after conversion
def fill_missing_values(row):
    # Check for NaT in 'first_review' and 'last_review'
    if pd.isnull(row['first_review']):
        row['first_review'] = pd.NaT  # Use NaT for missing datetime
    if pd.isnull(row['last_review']):
        row['last_review'] = pd.NaT  # Use NaT for missing datetime
    # Fill numeric fields with averages or default values
    if pd.isnull(row['number_of_reviews']):
        row['number_of_reviews'] = 0
    if pd.isnull(row['bedrooms']):
        row['bedrooms'] = average_bedrooms
    if pd.isnull(row['beds']):
        row['beds'] = average_beds
    if pd.isnull(row['security_deposit']):
        row['security_deposit'] = average_security_deposit
    if pd.isnull(row['cleaning_fee']):
        row['cleaning_fee'] = average_cleaning_fee
    return row

# Apply the function to fill missing values
df = df.apply(fill_missing_values, axis=1)

# Check the data types after handling dates
print("Data types after handling dates:")
print(df.dtypes)

# Convert back to Dataset format
dataset['train'] = Dataset.from_pandas(df)

# Verify the changes
print("Updated dataset sample:")
print(dataset['train'].to_pandas().head())

import pandas as pd

# Convert the dataset to a Pandas DataFrame
df = dataset['train'].to_pandas()

# Descriptive statistics for numeric fields
numeric_stats = df.describe()
print(numeric_stats)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Convert the dataset to a DataFrame from a specific split (assuming 'train' split exists)
df = dataset['train'].to_pandas()  # Adjust 'train' to your specific split name if different

# Set the style of seaborn
sns.set(style="whitegrid")

# List of numerical columns to visualize
numerical_columns = ['price', 'minimum_nights', 'maximum_nights',
                     'bedrooms', 'beds', 'number_of_reviews',
                     'bathrooms', 'security_deposit',
                     'cleaning_fee', 'extra_people',
                     'guests_included']

# Histograms
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot(4, 3, i)
    sns.histplot(df[col], bins=30, kde=True)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Box plots
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot(4, 3, i)
    sns.boxplot(y=df[col])
    plt.title(f'Box Plot of {col}')

plt.tight_layout()
plt.show()

# Scatter plot: Price vs Number of Reviews
plt.figure(figsize=(10, 6))
sns.scatterplot(x='number_of_reviews', y='price', data=df)
plt.title('Price vs Number of Reviews')
plt.xlabel('Number of Reviews')
plt.ylabel('Price')
plt.xscale('log')  # Use log scale for better visualization of outliers
plt.yscale('log')
plt.show()

from google.colab import files

# Save DataFrame to CSV
output_csv_path = 'cleaned_airbnb_data.csv'
df.to_csv(output_csv_path, index=False)

# Download the CSV file
files.download(output_csv_path)

"""Part 2"""

import pandas as pd
import ast  # To convert string representations of lists into actual lists

# Sample DataFrame setup (assuming 'df' is already loaded as your Airbnb dataset)
# Ensure 'amenities' is converted from a string representation to a list
if isinstance(df['amenities'].iloc[0], str):
    df['amenities'] = df['amenities'].apply(ast.literal_eval)

# Function 1: Filter by Amenities
def filter_by_amenities(df, required_amenities):
    """
    Filters listings based on the presence of required amenities.

    Args:
        df (DataFrame): The dataset to filter.
        required_amenities (list): List of amenities to filter by.

    Returns:
        DataFrame: Listings that have all the required amenities.
    """
    return df[df['amenities'].apply(lambda x: all(item in x for item in required_amenities))]

# Function 2: Filter by Numeric Fields (e.g., Beds, Price)
def filter_by_numeric_fields(df, min_beds=1, max_price=None):
    """
    Filters listings based on minimum number of beds and optional max price.

    Args:
        df (DataFrame): The dataset to filter.
        min_beds (int): Minimum number of beds required.
        max_price (int, optional): Maximum price for the listing.

    Returns:
        DataFrame: Filtered listings.
    """
    filtered_df = df[df['beds'] >= min_beds]
    if max_price is not None:
        filtered_df = filtered_df[filtered_df['price'] <= max_price]
    return filtered_df

# Function 3: Combine Filters for Advanced Queries
def filter_listings(df, required_amenities=[], min_beds=1, max_price=None):
    """
    Combines multiple filters to return listings matching all specified criteria.

    Args:
        df (DataFrame): The dataset to filter.
        required_amenities (list): List of required amenities.
        min_beds (int): Minimum number of beds.
        max_price (int, optional): Maximum price for the listing.

    Returns:
        DataFrame: Filtered listings.
    """
    if required_amenities:
        df = filter_by_amenities(df, required_amenities)
    df = filter_by_numeric_fields(df, min_beds, max_price)
    return df

# Function 4: Filter by Property Type
def filter_by_property_type(df, property_type):
    """
    Filters listings based on the property type.

    Args:
        df (DataFrame): The dataset to filter.
        property_type (str): The property type to filter by (e.g., 'Apartment', 'House').

    Returns:
        DataFrame: Filtered listings.
    """
    return df[df['property_type'] == property_type]

# Function 5: Filter by Availability
def filter_by_availability(df, min_availability=1):
    """
    Filters listings based on availability for a given number of days.

    Args:
        df (DataFrame): The dataset to filter.
        min_availability (int): Minimum number of days the listing must be available.

    Returns:
        DataFrame: Filtered listings.
    """
    return df[df['availability'] >= min_availability]

# Example usage of combined functions
result = filter_listings(
    df,
    required_amenities=['Wheelchair accessible', 'Elevator'],
    min_beds=2,
    max_price=150
)

# Display the filtered results
print("Filtered Listings:")
print(result.head())

import pandas as pd

# Sample data representing a simplified version of Airbnb listings
data = {
    '_id': [1, 2, 3, 4, 5],
    'property_type': ['Apartment', 'House', 'Condo', 'Apartment', 'Villa'],
    'beds': [2, 3, 1, 4, 2],
    'price': [120, 150, 90, 200, 130],
    'availability': [15, 30, 5, 20, 25],
    'amenities': [
        ['WiFi', 'Kitchen', 'Wheelchair accessible'],
        ['WiFi', 'Parking', 'Elevator'],
        ['WiFi', 'Kitchen'],
        ['WiFi', 'Kitchen', 'Elevator', 'Wheelchair accessible'],
        ['WiFi', 'Pool', 'Parking']
    ]
}

# Create a DataFrame from the sample data
df = pd.DataFrame(data)

# Display the sample data
print("Sample DataFrame:")
print(df)